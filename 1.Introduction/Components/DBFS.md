# ğŸ“‚ Databricks File System (DBFS)  

---

## ğŸ”¹ Introduction  
The **Databricks File System (DBFS)** is a **distributed file system** built on top of cloud storage (AWS S3, Azure Data Lake Storage, GCP GCS).  
It provides a **mount point** so you can interact with your cloud data just like a local file system.  

ğŸ‘‰ Think of DBFS as a **bridge between Spark and cloud storage**.  

---

## ğŸ§© Key Features  
- ğŸ“¦ **Storage Abstraction** â†’ Access cloud storage like local files.  
- âš¡ **Optimized for Spark** â†’ Fast read/write of big data.  
- ğŸ”’ **Secure Access** â†’ Controlled by Unity Catalog / IAM.  
- ğŸ”„ **Persistent Storage** â†’ Files remain even after cluster shutdown.  
- ğŸ› ï¸ **Mounting** â†’ Mount external cloud storage to `/mnt/`.  

---

## ğŸ“ DBFS Structure  

| Path                          | Description |
|-------------------------------|-------------|
| `/FileStore/`                 | Public files, images, ML models, notebooks export. |
| `/databricks-datasets/`       | Sample datasets provided by Databricks. |
| `/mnt/`                       | External storage mounts (S3, ADLS, GCS). |
| `/tmp/`                       | Temporary storage (cluster-specific). |
| `/user/hive/warehouse/`       | Hive tables & Delta tables storage. |

---

## ğŸ–¼ï¸ DBFS Design  

```

               â˜ï¸ Cloud Storage (S3 / ADLS / GCS)
                            â”‚
                            â–¼
                  ğŸ”— DBFS (Abstraction Layer)
                            â”‚

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ /FileStore   â”‚ /databricks- â”‚ /mnt         â”‚ /tmp         â”‚
â”‚              â”‚ datasets     â”‚ (mounts)     â”‚ (scratch)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
âš¡ Access via Spark / Python / SQL

````

---

## ğŸ§‘â€ğŸ’» Accessing DBFS  

### 1ï¸âƒ£ Using Databricks Utilities (`dbutils`)
```python
# List files
dbutils.fs.ls("/FileStore/")

# Copy file
dbutils.fs.cp("dbfs:/FileStore/data.csv", "dbfs:/mnt/raw/data.csv")

# Remove file
dbutils.fs.rm("dbfs:/tmp/sample.json")
````

---

### 2ï¸âƒ£ Using `%fs` Magic Command

```bash
%fs ls /databricks-datasets/
%fs head /FileStore/data.csv
```

---

### 3ï¸âƒ£ Using Spark APIs

```python
# Read CSV from DBFS
df = spark.read.csv("dbfs:/FileStore/data.csv", header=True, inferSchema=True)

# Write DataFrame to DBFS
df.write.format("delta").save("dbfs:/mnt/processed/sales_data")
```

---

### 4ï¸âƒ£ Mounting External Storage

```python
dbutils.fs.mount(
  source = "wasbs://container@storageaccount.blob.core.windows.net/",
  mount_point = "/mnt/raw",
  extra_configs = {"fs.azure.account.key.storageaccount.blob.core.windows.net":"<access-key>"}
)
```

---

## âœ… Benefits of DBFS

* ğŸ“‚ **Unified Access** â†’ Same interface across AWS, Azure, GCP.
* âš¡ **High Performance** â†’ Optimized for large-scale data workloads.
* ğŸ”— **Seamless Integration** â†’ Works with Spark, MLflow, Delta Lake.
* ğŸ”’ **Secure** â†’ Supports encryption and fine-grained access control.
* ğŸ› ï¸ **Developer Friendly** â†’ Access via Python, R, Scala, SQL, CLI.

---

## ğŸŒŸ Example Use Case

1. Upload raw CSV data to `/FileStore/`.
2. Run Spark ETL jobs on `/mnt/raw/`.
3. Save results in `/mnt/processed/` as Delta tables.
4. Train ML models using data stored in DBFS.
5. Share files via `/FileStore/` for visualization.

---
