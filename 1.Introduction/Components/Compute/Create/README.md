# âš™ï¸ Different Ways to Create Compute in Databricks  

---

## ğŸ”¹ Introduction  
Databricks provides multiple ways to **create compute resources** (clusters, SQL warehouses, job clusters, serverless compute, ML compute).  
Depending on your **persona** (Data Engineer, Data Scientist, Analyst, ML Engineer), you can create compute via **UI, API, CLI, or Infrastructure-as-Code (IaC)**.  

---

## ğŸ§© Ways to Create Compute  

### 1ï¸âƒ£ **Databricks UI (Workspace)** ğŸ–¥ï¸  
- Easiest & most common method.  
- Interactive web interface in the **Databricks Workspace**.  

âœ… Steps:  
1. Go to **Compute** in the left navigation bar.  
2. Select **Create Compute** (Cluster / SQL Warehouse / Job Cluster).  
3. Configure settings (runtime, workers, autoscaling, libraries).  
4. Launch the cluster or warehouse.  

ğŸ” Example: Creating an **interactive cluster** to run PySpark notebooks.  

---

### 2ï¸âƒ£ **Databricks CLI** ğŸ’»  
- Command-line tool to automate compute creation.  

âœ… Example: Create a cluster with a JSON config file.  

```bash
databricks clusters create --json '{
  "cluster_name": "demo-cluster",
  "spark_version": "11.3.x-scala2.12",
  "node_type_id": "Standard_DS3_v2",
  "num_workers": 2
}'
````

ğŸ” Best for: DevOps engineers automating environments.

---

### 3ï¸âƒ£ **Databricks REST API** ğŸŒ

* Direct API calls to programmatically create and manage compute.
* Flexible and integration-friendly (CI/CD pipelines).

âœ… Example: Create a job cluster via REST API.

```http
POST /api/2.0/clusters/create
Content-Type: application/json

{
  "cluster_name": "etl-job-cluster",
  "spark_version": "11.3.x-scala2.12",
  "node_type_id": "Standard_DS3_v2",
  "num_workers": 2
}
```

ğŸ” Best for: Automation with scripts or CI/CD pipelines.

---

### 4ï¸âƒ£ **Jobs UI / Workflows** ğŸ­

* When creating a **Job** in Databricks, you can configure a **Job Cluster**.
* The cluster will **start when the job runs** and **terminate automatically** after.

ğŸ” Best for: Production ETL, scheduled data pipelines.

---

### 5ï¸âƒ£ **Databricks SQL UI** ğŸ“Š

* In **Databricks SQL** workspace, you can create:

    * **Classic SQL Warehouses** (dedicated clusters).
    * **Serverless SQL Warehouses** (Databricks-managed).

ğŸ” Best for: BI Analysts running queries and dashboards.

---

### 6ï¸âƒ£ **Infrastructure as Code (IaC)** ğŸ—ï¸

* Use **Terraform, Azure ARM templates, AWS CloudFormation, or Databricks Terraform Provider**.
* Define compute resources as **code** for reproducibility.

âœ… Example (Terraform):

```hcl
resource "databricks_cluster" "this" {
  cluster_name  = "terraform-cluster"
  spark_version = "11.3.x-scala2.12"
  node_type_id  = "Standard_DS3_v2"
  num_workers   = 2
}
```

ğŸ” Best for: Enterprise environments with strict governance.

---

## ğŸ–¼ï¸ Visual Summary

```
     ğŸ–¥ï¸ UI (Workspace) â†’ For Data Engineers & Scientists (interactive)
     ğŸ’» CLI â†’ For DevOps automation
     ğŸŒ REST API â†’ For CI/CD pipelines
     ğŸ­ Jobs UI â†’ For ETL pipelines & workflows
     ğŸ“Š SQL UI â†’ For BI dashboards & analysts
     ğŸ—ï¸ IaC (Terraform/ARM/CloudFormation) â†’ For enterprise governance
```

---

## âœ… Recommendation

| Persona         | Preferred Way to Create Compute    |
| --------------- | ---------------------------------- |
| Data Engineer   | ğŸ–¥ï¸ Workspace UI, ğŸ­ Job Clusters  |
| Data Scientist  | ğŸ–¥ï¸ Interactive Clusters, ğŸ’» CLI   |
| BI Analyst      | ğŸ“Š SQL Warehouses (UI/Serverless)  |
| ML Engineer     | ğŸ¤– ML Clusters (UI + ML Runtime)   |
| DevOps Engineer | ğŸŒ REST API, ğŸ’» CLI, ğŸ—ï¸ Terraform |

---

ğŸš€ With these options, you can create **compute in Databricks** flexibly â€” from quick exploration to enterprise-scale production pipelines.

---
# âš¡ Special Parameters for Creating Different Types of Clusters using Databricks CLI

When creating clusters with the **Databricks CLI**, the JSON payload you pass to  
`databricks clusters create --json-file <file>.json`  
determines the **type of cluster** you get.  

Below are the **special parameters** needed for each cluster type:

---

## 1ï¸âƒ£ Interactive Cluster (for development & notebooks)
Interactive clusters are used for **ad-hoc exploration** and **notebook execution**.

```json
{
  "cluster_name": "dev-interactive-cluster",
  "spark_version": "12.2.x-scala2.12",
  "node_type_id": "Standard_DS3_v2",
  "num_workers": 2,
  "autotermination_minutes": 30,   // â³ auto-shutdown (optional)
  "cluster_source": "UI"           // âœ… Special parameter for interactive cluster
}
````

**ğŸ”‘ Special parameter:**

* `"cluster_source": "UI"` â†’ Marks the cluster as interactive (user-facing).

---

## 2ï¸âƒ£ Job (Automated) Cluster

Job clusters are **ephemeral** (created only for job runs and terminated afterward).

```json
{
  "cluster_name": "etl-job-cluster",
  "spark_version": "12.2.x-scala2.12",
  "node_type_id": "Standard_DS3_v2",
  "num_workers": 2,
  "cluster_source": "JOB"          // âœ… Special parameter for job cluster
}
```

**ğŸ”‘ Special parameter:**

* `"cluster_source": "JOB"` â†’ Marks the cluster as job-only.

---

## 3ï¸âƒ£ High-Concurrency Cluster

Designed for **multiple users / BI tools** (Databricks SQL, JDBC/ODBC).

```json
{
  "cluster_name": "bi-high-concurrency-cluster",
  "spark_version": "12.2.x-scala2.12",
  "node_type_id": "Standard_DS3_v2",
  "num_workers": 4,
  "spark_conf": {
    "spark.databricks.cluster.profile": "serverless"   // âœ… High concurrency config
  },
  "custom_tags": {
    "usage": "BI"
  }
}
```

**ğŸ”‘ Special parameter:**

* `"spark_conf": { "spark.databricks.cluster.profile": "serverless" }`
  â†’ Configures cluster for **high concurrency**.

---

## 4ï¸âƒ£ Serverless SQL Warehouse (via CLI)

Not exactly a cluster, but managed compute for SQL.
Created using **`databricks sql warehouses create`** instead of `clusters create`.

```json
{
  "name": "serverless-sql-wh",
  "cluster_size": "Small",
  "max_num_clusters": 1,
  "enable_serverless_compute": true   // âœ… Special parameter for serverless SQL
}
```

**ğŸ”‘ Special parameter:**

* `"enable_serverless_compute": true` â†’ Enables **serverless SQL Warehouse**.

---

## 5ï¸âƒ£ Single-Node Cluster

Useful for ML model training, lightweight tasks, or local-like execution.

```json
{
  "cluster_name": "ml-single-node-cluster",
  "spark_version": "12.2.x-scala2.12",
  "node_type_id": "Standard_DS3_v2",
  "num_workers": 0,                          // âœ… Must be 0
  "spark_conf": {
    "spark.master": "local[*]"               // âœ… Local mode
  }
}
```

**ğŸ”‘ Special parameters:**

* `"num_workers": 0` â†’ Forces single-node mode.
* `"spark_conf": { "spark.master": "local[*]" }`

---

## ğŸ“Š Summary Table

| Cluster Type      | Special Parameter(s)                                                 |
| ----------------- | -------------------------------------------------------------------- |
| Interactive       | `"cluster_source": "UI"`                                             |
| Job               | `"cluster_source": "JOB"`                                            |
| High-Concurrency  | `"spark_conf": { "spark.databricks.cluster.profile": "serverless" }` |
| Serverless SQL WH | `"enable_serverless_compute": true` (via SQL API, not clusters API)  |
| Single-Node       | `"num_workers": 0` + `"spark_conf": { "spark.master": "local[*]" }`  |

---

âœ… With these parameters, you can control exactly which **type of cluster** gets created when using the **Databricks CLI**.

---
