# âš¡ Autoloader in Databricks  

Databricks **Autoloader** is a feature that enables **incremental and efficient data ingestion** from cloud storage (AWS S3, Azure Data Lake, GCS) into **Delta Lake**.  
It **automatically detects and processes new files** as they arrive, without needing to reprocess older data.  

---

## ğŸš€ Key Features
- âœ… **Incremental loading** â€“ Only new files are processed.  
- âœ… **Schema evolution** â€“ Handles schema changes automatically.  
- âœ… **Scalability** â€“ Efficient for millions of files.  
- âœ… **Reliability** â€“ Tracks processed files using **checkpoints**.  
- âœ… **Integration** â€“ Works with **structured streaming** for real-time ingestion.  

---

## ğŸ—ï¸ How Autoloader Works
1. **Cloud Files Source** â†’ Monitors a directory in cloud storage.  
2. **Checkpoints** â†’ Tracks which files have already been ingested.  
3. **Schema Evolution** â†’ Adapts to new columns if schema changes.  
4. **Delta Lake Sink** â†’ Writes data into Delta tables for analytics.  

---

## ğŸ”‘ Syntax
```python
df = (spark.readStream
      .format("cloudFiles")                # Autoloader
      .option("cloudFiles.format", "json") # Input format (json, csv, parquet, etc.)
      .option("cloudFiles.schemaLocation", "/mnt/checkpoints/schema") # schema tracking
      .load("/mnt/raw_data/"))             # source folder
````

---

## ğŸ“¥ Example: Load JSON files into Delta Table

```python
# Read incremental data using Autoloader
df = (spark.readStream
      .format("cloudFiles")
      .option("cloudFiles.format", "json")
      .option("cloudFiles.schemaLocation", "/mnt/checkpoints/schema")
      .load("/mnt/raw/sales"))

# Write into Delta Table
(df.writeStream
   .format("delta")
   .option("checkpointLocation", "/mnt/checkpoints/sales")
   .outputMode("append")
   .table("bronze_sales"))
```

---

## ğŸ—‚ï¸ Schema Evolution Example

```python
df = (spark.readStream
      .format("cloudFiles")
      .option("cloudFiles.format", "csv")
      .option("cloudFiles.inferColumnTypes", "true") # auto infer schema
      .option("cloudFiles.schemaEvolutionMode", "addNewColumns")
      .option("cloudFiles.schemaLocation", "/mnt/checkpoints/schema")
      .load("/mnt/raw/customers"))
```

ğŸ‘‰ If a new column appears in future files, Autoloader will **automatically add it**.

---

## âš–ï¸ Two Modes of File Discovery

| Mode                  | Description                                                          | Best Use Case                             |
| --------------------- | -------------------------------------------------------------------- | ----------------------------------------- |
| **Directory Listing** | Scans directory for new files.                                       | Few files, small datasets.                |
| **File Notification** | Uses cloud-native event services (S3, ADLS, GCS) for file discovery. | Millions of files, large-scale ingestion. |

---

## ğŸ›ï¸ Autoloader in the Lakehouse

* **Bronze Layer** â†’ Ingest raw data incrementally.
* **Silver Layer** â†’ Apply transformations, cleansing.
* **Gold Layer** â†’ Aggregated, business-ready data.

---

## ğŸ¯ Benefits of Autoloader

* No need for **manual scheduling** of ingestion jobs.
* Saves **compute costs** by avoiding reprocessing.
* Provides **real-time data availability**.
* Handles **schema drift** with minimal effort.

---

ğŸ”¹ **In summary:**
Autoloader = *Smart, scalable, and incremental data ingestion into Delta Lake with minimal configuration*.