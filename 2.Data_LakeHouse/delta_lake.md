# ğŸŒŠ Delta Lake in Databricks  

Delta Lake is an **open-source storage layer** that brings **reliability, performance, and ACID transactions** to data lakes.  
It is tightly integrated with Databricks and enables the **Lakehouse architecture**.  

---

## ğŸ—ï¸ What is Delta Lake?  

- âœ… Built on top of **Apache Parquet** files  
- âœ… Ensures **ACID transactions** (Atomicity, Consistency, Isolation, Durability)  
- âœ… Provides **schema enforcement & evolution**  
- âœ… Enables **time travel** (querying older versions of data)  
- âœ… Supports **unified batch & streaming**  

---

## ğŸ”‘ Key Features  

| Feature | Description | Example |
|---------|-------------|---------|
| **ACID Transactions** | Guarantees reliable writes & reads | Multiple users can read/write safely |
| **Schema Enforcement** | Prevents bad/incorrect data | Rejects invalid schema writes |
| **Schema Evolution** | Allows schema updates | Add new columns dynamically |
| **Time Travel** | Query old versions of data | `VERSION AS OF` |
| **Scalable Metadata** | Stores metadata in Delta logs | Faster than Hive Metastore |
| **Batch + Streaming** | Supports both workloads | Structured Streaming with Delta |

---

## ğŸ“Š Delta Lake Architecture  

```
         +-----------------------------+
         |       Delta Lake Table       |
         +-----------------------------+
         | Parquet Files (Data Storage) |
         | Delta Log (_delta_log JSON)  |
         +-----------------------------+
```
```
ğŸ“ /mnt/delta/events
â”œâ”€â”€ part-0001.snappy.parquet <- Data File
â”œâ”€â”€ part-0002.snappy.parquet <- Data File
â”œâ”€â”€ _delta_log/ <- Transaction Log
â”‚ â”œâ”€â”€ 00000000000000000001.json <- Transaction #1
â”‚ â”œâ”€â”€ 00000000000000000002.json <- Transaction #2
â”‚ â”œâ”€â”€ 00000000000000000003.checkpoint.parquet
â”‚ â””â”€â”€ ...
â””â”€â”€ ...
```

- **Parquet files** â†’ Store the actual data  
- **Delta Log (`_delta_log`)** â†’ Stores metadata, schema changes, and transactions  

---

## âš™ï¸ Delta Lake Commands  

### 1. **Create a Delta Table**
```python
data = spark.range(0, 5)
data.write.format("delta").save("/mnt/delta/events")
````

---

### 2. **Read a Delta Table**

```python
df = spark.read.format("delta").load("/mnt/delta/events")
df.show()
```

---

### 3. **Update Data**

```python
from delta.tables import DeltaTable

deltaTable = DeltaTable.forPath(spark, "/mnt/delta/events")
deltaTable.update(
    condition = "id = 2",
    set = { "id": "200" }
)
```

---

### 4. **Delete Data**

```python
deltaTable.delete("id = 3")
```

---

### 5. **Merge (Upsert)**

```python
newData = spark.createDataFrame([(2,), (5,)], ["id"])

deltaTable.alias("old").merge(
    newData.alias("new"),
    "old.id = new.id"
).whenMatchedUpdate(set = { "id": "new.id" }) \
 .whenNotMatchedInsert(values = { "id": "new.id" }) \
 .execute()
```

---

### 6. **Time Travel**

```python
# Query by version
df = spark.read.format("delta").option("versionAsOf", 1).load("/mnt/delta/events")

# Query by timestamp
df = spark.read.format("delta").option("timestampAsOf", "2025-08-25 12:00:00").load("/mnt/delta/events")
```

---

## ğŸš€ Benefits of Delta Lake in Databricks

* ğŸ”¹ **Data Reliability** â†’ Handles concurrent reads/writes safely
* ğŸ”¹ **Faster Queries** â†’ Optimized storage & caching
* ğŸ”¹ **Cost-Effective** â†’ Store raw + curated data in the same place
* ğŸ”¹ **Governance Ready** â†’ Works with **Unity Catalog** for access control
* ğŸ”¹ **Streaming + Batch Together** â†’ No need for separate architectures

---

## ğŸ† Use Cases

* ğŸ“‚ **Data Lakehouse** (Unified Data Lake + Warehouse)
* ğŸ“ˆ **ETL Pipelines** (Incremental batch processing)
* â³ **Historical Analysis** (using Time Travel)
* âš¡ **Real-Time Analytics** (streaming with Delta)
* ğŸ” **ML Feature Store** (storing curated ML features)

---

âœ… Delta Lake is the **foundation of Databricks Lakehouse**: it combines the **scalability of data lakes** with the **performance of warehouses**.

